---
title: "Sums of squares"
author: "Dr. Brinda"
date: "September 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Categorical variable
1. visualize time series
2. scatter plot separated by month
3. scatter plot - log(y) separated by month
4. build categorical design matrix with dummy variables sum of row always=1 every category will have 1 prediction value -> ms <- d\times 1


Recall our airline passenger data.


```{r}
y <- AirPassengers
x <- seq(1949, 1960+11/12, by=1/12)

plot(x, y, type="l", xlab="time", ylab="thousands of international airline passengers")
```


Let's use this same dataset to explore the use of linear regression for categorical data by considering the months (Jan, Feb, ..., Dec) as 12 groups.

```{r}
# The 12 groups are balanced
y

# so the number of observations per group is
n0 <- length(y)/12

plot(rep(1:12, n0), y, xlab="month", ylab="thousands of international airline passengers")
```

Notice that the groups with larger values tend to also have larger variances; this is common and generally undesirable for  a variety of reasons, some of which we'll see in future chapters.

Often a log transformation of the response variable results in groups with more similar variance.

```{r}
plot(rep(1:12, n0), log(y), xlab="month", ylab="log of thousands of international airline passengers")
```

That's better. Let's use $\log(y)$ as our response variable instead.

```{r}
logy <- log(y)
```

Now I'm going to construct a design matrix then use our least-squares formula to find a coefficient for each group.

```{r}
jan <- rep(c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), n0)
feb <- rep(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), n0)
mar <- rep(c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), n0)
apr <- rep(c(0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), n0)
may <- rep(c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), n0)
jun <- rep(c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), n0)
jul <- rep(c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), n0)
aug <- rep(c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), n0)
sep <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0), n0)
oct <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0), n0)
nov <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), n0)
dec <- rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), n0)

X <- cbind(jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec)
ms <- solve(t(X) %*% X) %*% t(X) %*% logy

# the prediction vector cycles through these means each year
logy.hat <- X %*% solve(t(X) %*% X) %*% t(X) %*% logy
```

The least-squares coefficients are exactly the groups' averages. Let's add them to our dot plot.

```{r}
plot(rep(1:12, length(y)/12), logy, xlab="month", ylab="log of thousands of international airline passengers")
points(1:12, ms, col="red", pch=5, cex=2)
```




## Decomposing sums of squares with contrasts


The squared length of the regression vector $\mathbf{\hat{y}} - \mathbf{\bar{y}}$ (aka "regression sum of squares") is

```{r}
sum((logy.hat - mean(logy))^2)
```


Because the groups are balanced, we can use contrasts to further decompose the regression sum of squares. (For now, the contrasts are just giving us an orthogonal basis for the orthogonal complement of $\mathbf{1}$ in $C(\mathbb{X})$. Later in this course, contrast vectors will be designed with specific interpretations in mind.)

Below, I've defined eleven orthogonal contrasts for us. 

```{r}
c1 <- c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
c2 <- c(0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0)
c3 <- c(0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0)
c4 <- c(0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0)
c5 <- c(0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0)
c6 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)

c7 <- c(1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0)
c8 <- c(0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0)
c9 <- c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, -1)

c10 <- c(1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1, -1)

c11 <- c(1, 1, 1, 1, -2, -2, -2, -2, 1, 1, 1, 1)

# check that they're orthogonal to each other and to 1
C <- cbind(1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11)
t(C) %*% C
ms <- solve(t(X) %*% X) %*% t(X) %*% logy
# The corresponding vectors in R^n are, for example,
c1x <- X %*% c1
# but you won't actually need these.
```




### Task 1 (contrasts)


Let's use the contrast formula that we derived in Monday's lecture (and at the end of Chapter 2 of the textbook).

```{r}

squared.coeffs <- rep(0, 11)
for(j in 1:11) {
  contrast <- C[, j+1]
  squared.coeffs[j] <- n0*(contrast%*%ms)^2/sum(contrast^2)
  # CALCULATE THE CONTRASTS' SQUARED COEFFICIENT
  # USING THE FORMULA THAT WE DERIVED IN CLASS (and in the text).
  # IT SHOULD ONLY INVOLVE "contrast" AND "ms" AND "n0".
  # SAVE THE RESULT IN squared.coeffs[j]
}
squared.coeffs
```


Let's verify that the sum of these squared coefficients equals the regression sum of squares.

```{r}
sum(squared.coeffs)
```





## Mixing quantitative and categorical variables





The categorical variable for month can capture the seasonal pattern, but it's missing the overall trend.

```{r}
plot(x, logy, type="l", xlab="time", ylab="log of thousands of international airline passengers")
lines(x, logy.hat, col="red")
```

On the other hand, our earlier quadratic fit captured the overall trend but ignored the seasonal pattern. Let's finally put everything together to fit this data well.


### Task 2 (a design matrix)


Let's find the least-squares fit for $\log(y)$ that includes a slope for the centered time variable `x - mean(x)` but allows each month to have its own intercept.

In other words, the set of possible fits are functions of the form
\[
f(x) = a_1 I(m(x) = \text{Jan}) + a_2 I(m(x) = \text{Feb}) + a_3 I(m(x) = \text{Mar}) + a_4 I(m(x) = \text{Apr}) + a_5 I(m(x) = \text{May}) + a_6 I(m(x) = \text{Jun}) + a_7 I(m(x) = \text{Jul})\\
+ a_8 I(m(x) = \text{Aug}) + a_9 I(m(x) = \text{Sep}) + a_{10} I(m(x) = \text{Oct}) + a_{11} I(m(x) = \text{Nov}) + a_{12} I(m(x) = \text{Dec}) + b_1 (x - \bar{x})
\]
where $m(x)$ represents the month and $a_1, \ldots, a_{12}, b_1$ are the free parameters. Because the space of possible fits is linear in the parameters, the least-squares fit coincides with the orthogonal projection and can be found using our usual formulas.

> Define a design matrix `X2` that corresponds to this space of possible fits.

```{r}

# X2 <- cbind( FIGURE OUT WHAT TO PUT IN HERE )

# PLACEHOLDER, JUST SO THAT YOU WON'T HAVE
# KNITTING ERRORS BEFORE YOU GET TO THIS PART:
X2 <- cbind(jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec,x - mean(x))
# MAKE SURE TO DELETE THIS WHEN YOU DEFINE THE REAL X2
```


Below, I've calculated the resulting least-squares predicted values to see how well they match the data.

```{r}
logy.hat2 <- X2 %*% solve(t(X2) %*% X2) %*% t(X2) %*% logy

plot(x, logy, type="l", xlab="time", ylab="log of thousands of international airline passengers")
lines(x, logy.hat2, col="red")
```



### Task 3 (another design matrix)

Let's find the least-squares fit for $\log(y)$ that includes a quadratic fit with the centered time variable `x - mean(x)` but allows each month to have its own intercept.

In other words, the set of possible fits are functions of the form
\[
f(x) = a_1 I(m(x) = \text{Jan}) + a_2 I(m(x) = \text{Feb}) + a_3 I(m(x) = \text{Mar}) + a_4 I(m(x) = \text{Apr}) + a_5 I(m(x) = \text{May}) + a_6 I(m(x) = \text{Jun}) + a_7 I(m(x) = \text{Jul})\\
+ a_8 I(m(x) = \text{Aug}) + a_9 I(m(x) = \text{Sep}) + a_{10} I(m(x) = \text{Oct}) + a_{11} I(m(x) = \text{Nov}) + a_{12} I(m(x) = \text{Dec}) + b_1 (x - \bar{x}) + b_2 (x - \bar{x})^2.
\]

> Define a design matrix `X3` that corresponds to this space of possible fits.

```{r}

# X3 <- cbind( FIGURE OUT WHAT TO PUT IN HERE )


# PLACEHOLDER, JUST SO THAT YOU WON'T HAVE
# KNITTING ERRORS BEFORE YOU GET TO THIS PART:
X3 <- cbind(jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec,x - mean(x), (x - mean(x))^2)
# MAKE SURE TO DELETE THIS WHEN YOU DEFINE THE REAL X3
```


Below, you'll see how well the resulting least-squares predicted values match the data.

```{r}
logy.hat3 <- X3 %*% solve(t(X3) %*% X3) %*% t(X3) %*% logy

plot(x, logy, type="l", xlab="time", ylab="log of thousands of international airline passengers")
lines(x, logy.hat3, col="red")
```



### Task 4 (decomposing regression sum of squares)



> Use the Pythagorean identity to find a vector in $\mathbb{R}^n$ whose squared length is equal to the following.

```{r}
sum((logy.hat3 - mean(logy))^2) - sum((logy.hat2 - logy.hat)^2) - sum(squared.coeffs)
sum((logy.hat3 - logy.hat2)^2)
```

> Explain your answer briefly here.
Let $y_3$ denotes the orthogonal projection of $y$ on to $span \{jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec,x - \bar{x}, (x - \bar{x})^2 \}$,$y_2$ denotes the orthogonal projection of $y$ on to $span \{ jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec,x - \bar{x} \}$,$y_1$ denotes the orthogonal projection of $y$ on to $span \{ jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec \}$,then we have $y_2-y_1$,$y_1-\bar{y}$ are orthogonal to each other and $y_3-y_2$,$y_2-y_1$ are orthogonal to each other.
According to the Pythagorean theorem,
$$
\|\mathbf{y}_{\mathcal{3}}-\mathbf{\bar y}\|^{2}=\left\|\mathbf{y}_{\mathcal{2}}-\mathbf{y}_{\mathcal{1}}\right\|^{2}+\left\|\mathbf{y}_{\mathcal{1}}-\mathbf{\bar y}\right\|^{2}+\left\|\mathbf{y}_{\mathcal{3}}-\mathbf{y}_{\mathcal{2}}\right\|^{2}
$$
$$
\|\mathbf{y}_{\mathcal{3}}-\mathbf{\bar y}\|^{2}-\left\|\mathbf{y}_{\mathcal{2}}-\mathbf{y}_{\mathcal{1}}\right\|^{2}-\left\|\mathbf{y}_{\mathcal{1}}-\mathbf{\bar y}\right\|^{2}=\left\|\mathbf{y}_{\mathcal{3}}-\mathbf{y}_{\mathcal{2}}\right\|^{2}
$$

```{r}
# Verify your answer

# sum(( PUT THE VECTOR HERE )^2)


```





### Task 5 (fitting the untransformed data)


We've used linear regression to fit the log of the number of passengers. There's a *natural* way of transforming your predicted values `logy.hat3` in order to fit the original untransformed response data.

Use the `lines` function to draw these transformed fitted values onto the plot below.

```{r}
plot(x, y, type="l", xlab="time", ylab="thousands of international airline passengers")
lines(x, exp(logy.hat3), col="red")

```



